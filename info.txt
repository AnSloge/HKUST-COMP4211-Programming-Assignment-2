Machine learning assignment criterias!

1. Set random state in models for reproductability
2. always include question number and description (commented out)
3. Try not to overfit or underfit data.
4. Focus on not making the same prediciton for every single image (previous problem)  

Exercise description from PDF file:

[Q1] Showcase some samples from the training dataset, with at least 3 images per class. Show
each image with its corresponding class label.
The code for downloading and preprocessing the images has been provided in the Jupyter note-
book.


MODEL INFO START

For the classification model we will use the Wide ResNet architecture (WRN), a variant of the
ResNet model that focuses on width (number of channels) over depth (number of layers). The
particular configuration of the model to be used is called WRN-16-4:

The model takes in an image (assumed to be of size 32 × 32) and outputs a single vector
containing the logits for the class labels.

MODEL INFO END


RES NET BLOCK INFO START

A ResNet consists of multiple “stages” of processing, where the feature map is progressively
downsampled in size and widened in the number of channels. Each stage consists of a stack of
ResNet blocks, usually with the same number of output channels, and the first block in each
stage also performing downsampling

Some details on the implementation of the ResNet block in this model:
•Convolution Layers: The F(x) part of this model consists of 4 layers: 2 LeakyReLU
activations (default setting), and 2 3×3 Convolution, with the ‘same’ padding.
The number of output channels (out channels) should be the same for both Conv2D
layers, with the LeakyReLU passed before each convolution.
Note: This implementation does not use any normalization layer, unlike the original
proposed model.

•Skip Connection: The skip connection (+x part) should be an identity function (i.e.,
the original input, unchanged) if x and F(x) have the same dimensionality (shape).
If the dimensionalities are different, then the skip connection should use a 1×1 convolution
mapping from (in channels) to (out channels) channels, with no bias. Then the output
should look like F(x) + skip(x).
•Downsampling: Downsampling, if needed, is handled using strides. Set strides = 2
on the second convolution layer to perform downsampling.
Since this modifies the F(x) dimensionality, the same stride setting should also be added
in the skip connection convolution.

* [C1] Create a Keras Layer implementing the ResNet Block.

* [Q2] [C2] Create a Keras Model implementing the WRN-16-4 Model. Report the number of
trainable parameters.
Tip: The WRN model can be implemented with the Keras Sequential function, using the
ResNet Blocks

RES NET BLOCK INFO END


TRAINING ROUTING START 

For this task, you will implement a custom training step for the model, which a skeleton is
provided for you as part1 train step(). The training step function takes in an optimizer
(optim), a classification model, a batch of image data and label.
The training step function should consist of the following steps:
1. Convert the label into a one-hot encoding, and augment the image data x by adding
Gaussian noise of standard deviation sigma (default 0.03).
2. With a GradientTape:
(a) Compute the logits for the image batch using the model.
(b) Compute the categorical Cross Entropy loss by comparing the logits with the ground
truth.
3. Obtain the gradients of the loss with respect to the model’s trainable variables from the
gradient tape.
4. Apply the gradients using the optimizer. This is where the model learns by updating its
weights based on the computed gradients.
5. Return the loss obtained, as a dictionary. These will be displayed during the training
process bar in real time.
You are allowed to return different values for monitoring purposes, but you must map the
key ’loss’ to the total loss.

*[Q3] What is the use of adding Gaussian noise to the image data?

*[C3] Implement the training step function part1 train step() given the above instructions.

TRAINING ROUTING END 


TRAINIG LOOP START

The main training loop function has been provided to you (train loop 1()). This function
takes in a model, an optimizer, a train step function, the number of epochs, the interval
between weight saves.
It performs the following:
•Load the data batches and pass the corresponding arguments to the supplied train step
function.
•Obtain the loss from the training step, accumulating them and reporting the current loss
using the tqdm progress bar.
•Report the average loss in each epoch, and optionally, the test accuracy of the model.
You can uncomment the “Test Accuracy” section to track your model’s performance,
provided you have implemented 5.4.
•Save the model every few epochs for restoration if something goes wrong during training.

*[Q4] Train your model with the above task for at least 20 epochs. Report the loss for at least
the first and last epochs of training

TRAINIG LOOP END


TESTING THE MODEL START


* [C4] Implement a testing function evaluate accuracy that evaluates the accuracy of the model
in classifying a given dataset.
* [Q5] Report the accuracy of the model trained in the previous section, tested against the test
dataset. Showcase at least 4 misclassified samples (along with the predicted and ground truth
label)

TESTING THE MODEL END


REGULARIZATION START

Regularization is widely used to avoid the overfitting issues in the training of neural net-
works.
* [C5] Choose ONE regularization method, such as Dropout or Batch Normalization, and im-
plement it in the neural network you have trained. For Dropout, refer to Section 2.4 in https://
arxiv.org/pdf/1605.07146. For Batch Normalization, refer to https://www.neuralception.
com/objectdetection-batchnorm/. Modify the model class ResBlockRE, train your modified
model on the same task for at least 20 epochs, and test it on the test dataset as before.
* [Q6] Report the accuracy of the modified model when tested against the test dataset. Com-
pare the performance with and without regularization, and briefly describe your findings and
explanations.

REGULARIZATION END


